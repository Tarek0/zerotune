{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## configuration\n",
        "\n",
        "DECISION_TREE_CL_CONFIG = {\"name\": \"DecisionTreeClassifier\",\n",
        "                           \"model\": DecisionTreeClassifier(random_state=42),\n",
        "                           'metric': 'roc_auc',\n",
        "                           'param_config':{\n",
        "                               'max_depth': {'percentage_splits': [0.25, 0.50, 0.70, 0.8, 0.9, 0.999], 'param_type':\"int\", 'dependency':'n_samples'},\n",
        "                               'min_samples_split': {'percentage_splits': [0.005, 0.01, 0.02, 0.05, 0.10], 'param_type':\"float\"},\n",
        "                               'min_samples_leaf': {'percentage_splits': [0.005, 0.01, 0.02, 0.05, 0.10], 'param_type':\"float\"},\n",
        "                               'max_features': {'percentage_splits': [0.50, 0.70, 0.8, 0.9, 0.99], 'param_type':\"float\"}\n",
        "                           }\n",
        "                          }\n",
        "\n",
        "\n",
        "ml_config = DECISION_TREE_CL_CONFIG_4PARAM\n",
        "print(ml_config)"
      ],
      "metadata": {
        "id": "DjJq0fwftKap"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "FPSfJ2LitDVs",
        "outputId": "7d027f40-778c-43e3-d9d8-94cea31f0511"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c9f64174b238>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openml'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.pipeline import Pipeline\n",
        "import openml\n",
        "import random\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "exp_id = '20240812' #'20240323' #'20240216'\n",
        "output_root = f'./output/{exp_id}/'\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "\n",
        "# core knowledge base functions\n",
        "def fetch_open_ml_data(dataset_id):\n",
        "\n",
        "    dataset = openml.datasets.get_dataset(dataset_id)\n",
        "\n",
        "    print(f'Dataset name: {dataset.name}')\n",
        "\n",
        "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
        "        dataset_format=\"array\", target=dataset.default_target_attribute\n",
        "    )\n",
        "    df = pd.DataFrame(X, columns=attribute_names)\n",
        "    df[\"target\"] = y\n",
        "\n",
        "    return df, 'target', dataset.name\n",
        "\n",
        "def prepare_data(df, target_name):\n",
        "    \"\"\"Simple preprocessing wrapper function\n",
        "    :param df. pandas dataframe containing dataset\n",
        "    :param target_name. the name of the target variable column\n",
        "    :return data dict containing the preprocessed pandas dataframes\"\"\"\n",
        "    y = df[target_name]\n",
        "    X = df.drop(target_name, axis=1)\n",
        "    X.fillna(0, inplace=True)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def _calculate_dataset_size(X):\n",
        "    return {\"n_samples\": X.shape[0], \"n_features\": X.shape[1]}\n",
        "\n",
        "def _calculate_class_imbalance_ratio(y):\n",
        "    \"\"\"\n",
        "    Calculate the class imbalance ratio of a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    y: array-like, shape (n_samples,)\n",
        "       Target values (class labels).\n",
        "\n",
        "    Returns:\n",
        "    float: The ratio of the majority class size to the minority class size.\n",
        "    \"\"\"\n",
        "    # Count the occurrences of each class\n",
        "    class_counts = np.bincount(y)\n",
        "\n",
        "    # Find the counts of majority and minority classes\n",
        "    majority_class_count = np.max(class_counts)\n",
        "    minority_class_count = np.min(class_counts)\n",
        "\n",
        "    # Calculate the imbalance ratio\n",
        "    imbalance_ratio = majority_class_count / minority_class_count\n",
        "\n",
        "    return {\"imbalance_ratio\": imbalance_ratio}\n",
        "\n",
        "def _calculate_correlation_metrics(X, y, correlation_cutoff=0.1):\n",
        "    \"\"\"\n",
        "    Calculates and returns correlation metrics between features in X and the target variable y,\n",
        "    filtering for features that have a correlation above a specified cutoff.\n",
        "\n",
        "    Parameters:\n",
        "    - X (array-like, DataFrame): The input features, where rows represent samples and columns represent features.\n",
        "    - y (array-like, Series): The target variable for which correlations with features in X are computed.\n",
        "    - correlation_cutoff (float, optional): The minimum absolute correlation value for a feature to be considered\n",
        "      informative with respect to the target variable. Defaults to 0.1.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing the following key-value pairs:\n",
        "        - 'n_highly_target_corr': The number of features that have an absolute correlation with the target\n",
        "          variable greater than the specified cutoff.\n",
        "        - 'avg_target_corr': The average absolute correlation of all features with the target variable.\n",
        "        - 'var_target_corr': The variance of the absolute correlations of all features with the target variable.\n",
        "\n",
        "    Note:\n",
        "    - The function uses Pearson correlation by default but can be adjusted to use 'spearman' or 'kendall'\n",
        "      for non-linear relationships by modifying the `corr` method call.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(X.copy())\n",
        "    df['target'] = pd.Series(y.copy())\n",
        "    correlation_matrix = df.corr(method='pearson')  # Use 'spearman' or 'kendall' for non-linear relationships\n",
        "    correlations_with_target = abs(correlation_matrix['target'])\n",
        "\n",
        "    informative_features = correlations_with_target[correlations_with_target > correlation_cutoff].sort_values(ascending=False)\n",
        "    n_informative = len(informative_features) - 1\n",
        "\n",
        "    return {'n_highly_target_corr': n_informative  # Number of highly target correlated features\n",
        "            , 'avg_target_corr' : correlations_with_target.mean() # Avg target correlation\n",
        "            , 'var_target_corr' : correlations_with_target.var() # Variance of target correlation\n",
        "           }\n",
        "\n",
        "def _calculate_feature_moments_and_variances(X):\n",
        "    \"\"\"\n",
        "    Calculate the first four statistical moments (mean, variance, skewness, kurtosis), their averages,\n",
        "    and variances for each feature in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    X (DataFrame): DataFrame containing the feature set.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with the first four moments, their averages, and variances for each feature.\n",
        "    \"\"\"\n",
        "    # Converting to DataFrame if not already (for compatibility)\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X)\n",
        "\n",
        "    # Calculate first moment (mean) for each col\n",
        "    moment_1 = X.apply(lambda x: x.mean(), axis=0)\n",
        "\n",
        "    # Calculate second moment (variance) for each col\n",
        "    moment_2 = X.apply(lambda x: x.var(), axis=0)\n",
        "\n",
        "    # Calculate third moment (skewness) for each col\n",
        "    moment_3 = X.apply(lambda x: skew(x.dropna()), axis=0)\n",
        "\n",
        "    # Calculate fourth moment (kurtosis) for each col\n",
        "    moment_4 = X.apply(lambda x: kurtosis(x.dropna()), axis=0)\n",
        "\n",
        "    # Calculate and add the averages and variances of all moments\n",
        "    moments = {'avg_feature_m1': moment_1.mean()  # Average Mean\n",
        "              , 'var_feature_m1': moment_1.var()   # Variance of Mean\n",
        "              , 'avg_feature_m2': moment_2.mean()  # Average Variance\n",
        "              , 'var_feature_m2': moment_2.var()   # Variance of Variance\n",
        "              , 'avg_feature_m3': moment_3.mean()  # Average Skewness\n",
        "              , 'var_feature_m3': moment_3.var()   # Variance of Skewness\n",
        "              , 'avg_feature_m4': moment_4.mean()  # Average Kurtosis\n",
        "              , 'var_feature_m4': moment_4.var()   # Variance of Kurtosis\n",
        "              }\n",
        "\n",
        "    return moments\n",
        "\n",
        "def _calculate_row_moments_and_variances(X):\n",
        "    \"\"\"\n",
        "    Calculate the first four statistical moments (mean, variance, skewness, kurtosis), their averages,\n",
        "    and variances for each row in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    X (DataFrame): DataFrame containing the feature set.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with the first four moments, their averages, and variances for each row.\n",
        "    \"\"\"\n",
        "    # Converting to DataFrame if not already (for compatibility)\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X)\n",
        "\n",
        "    # Calculate first moment (mean) for each row\n",
        "    moment_1 = X.apply(lambda x: x.mean(), axis=1)\n",
        "\n",
        "    # Calculate second moment (variance) for each row\n",
        "    moment_2 = X.apply(lambda x: x.var(), axis=1)\n",
        "\n",
        "    # Calculate third moment (skewness) for each row\n",
        "    moment_3 = X.apply(lambda x: skew(x.dropna()), axis=1)\n",
        "\n",
        "    # Calculate fourth moment (kurtosis) for each row\n",
        "    moment_4 = X.apply(lambda x: kurtosis(x.dropna()), axis=1)\n",
        "\n",
        "    # Calculate and add the averages and variances of all moments\n",
        "    moments = {'avg_row_m1': moment_1.mean()  # Average Mean\n",
        "              , 'var_row_m1': moment_1.var()   # Variance of Mean\n",
        "              , 'avg_row_m2': moment_2.mean()  # Average Variance\n",
        "              , 'var_row_m2': moment_2.var()   # Variance of Variance\n",
        "              , 'avg_row_m3': moment_3.mean()  # Average Skewness\n",
        "              , 'var_row_m3': moment_3.var()   # Variance of Skewness\n",
        "              , 'avg_row_m4': moment_4.mean()  # Average Kurtosis\n",
        "              , 'var_row_m4': moment_4.var()   # Variance of Kurtosis\n",
        "              }\n",
        "\n",
        "    return moments\n",
        "\n",
        "def _calculate_skewness_kurtosis_stats(X):\n",
        "    \"\"\"\n",
        "    Calculate the skewness and kurtosis for each numerical feature in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    X (DataFrame): DataFrame containing the feature set.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with skewness and kurtosis for each feature.\n",
        "    \"\"\"\n",
        "    # Converting to DataFrame if not already (for compatibility)\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X)\n",
        "\n",
        "    # Calculate skewness and kurtosis for each feature\n",
        "    skewness = X.apply(lambda x: skew(x.dropna()), axis=0)\n",
        "    kurtosis_values = X.apply(lambda x: kurtosis(x.dropna()), axis=0)\n",
        "\n",
        "    # Calculate average and standard deviation of skewness and kurtosis\n",
        "    skewness_kurtosis_stats = {\n",
        "        'average_skewness': skewness.mean(),\n",
        "        'std_dev_skewness': skewness.std(),\n",
        "        'average_kurtosis': kurtosis_values.mean(),\n",
        "        'std_dev_kurtosis': kurtosis_values.std()\n",
        "    }\n",
        "\n",
        "    return skewness_kurtosis_stats\n",
        "\n",
        "\n",
        "def calculate_dataset_meta_parameters(X, y):\n",
        "\n",
        "    meta_parameters = {}\n",
        "    meta_parameters.update( _calculate_dataset_size(X) )\n",
        "    meta_parameters.update( _calculate_class_imbalance_ratio(y) )\n",
        "    meta_parameters.update( _calculate_correlation_metrics(X, y, correlation_cutoff=0.10) )\n",
        "    meta_parameters.update( _calculate_feature_moments_and_variances(X) )\n",
        "    meta_parameters.update( _calculate_row_moments_and_variances(X) ) # experimental\n",
        "\n",
        "    return meta_parameters\n",
        "\n",
        "\n",
        "def _relative2absolute_dict(param_config, dataset_properties, param_dict):\n",
        "    # Create a copy of the param_dict to avoid modifying the original\n",
        "    absolute_param_dict = param_dict.copy()\n",
        "\n",
        "    params_with_dependency = [param for param, details in param_config.items() if 'dependency' in details]\n",
        "    for p in params_with_dependency:\n",
        "        dependency_col = param_config[p]['dependency']\n",
        "        dependency_value = dataset_properties[dependency_col]\n",
        "        absolute_param_dict[p] = max(int(dependency_value * absolute_param_dict[p]), 1)\n",
        "\n",
        "    return absolute_param_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HPO training functions\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import Trial\n",
        "\n",
        "\n",
        "def evaluate_model(X, y, model_name, hyperparams, random_seed=42, n_folds=3, n_seeds=20):\n",
        "    seed_scores = []\n",
        "\n",
        "    for i in range(n_seeds):\n",
        "        seed = random_seed + i\n",
        "        if model_name == \"DecisionTreeClassifier\":\n",
        "            model = DecisionTreeClassifier(random_state=seed, **hyperparams)\n",
        "        elif model_name == \"RandomForestClassifier\":\n",
        "            model = RandomForestClassifier(random_state=seed, **hyperparams)\n",
        "        elif model_name == \"XGBClassifier\":\n",
        "            model = XGBClassifier(random_state=seed, **hyperparams)\n",
        "\n",
        "        cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "        scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
        "        seed_scores.append(np.mean(scores))\n",
        "\n",
        "    final_score = np.mean(seed_scores)\n",
        "    return final_score, seed_scores\n",
        "\n",
        "def _optuna_objective(trial: Trial, X, y, param_config, meta_params, dataset_meta_params, random_seed=42):\n",
        "\n",
        "    # Generate hyperparameters based on the trial\n",
        "    hyperparams = {}\n",
        "    for param, config in param_config.items():\n",
        "        if 'percentage_splits' in config:\n",
        "            min_value = min(config['percentage_splits'])\n",
        "            max_value = max(config['percentage_splits'])\n",
        "            hyperparams[param] = trial.suggest_uniform(param, min_value, max_value)\n",
        "        # Add other parameter types (e.g., suggest_int, suggest_loguniform) as needed\n",
        "\n",
        "    predicted_hyperparams = _relative2absolute_dict(param_config, dataset_meta_params, hyperparams)\n",
        "\n",
        "    score, _ = evaluate_model(X, y, \"DecisionTreeClassifier4Param\", predicted_hyperparams, random_seed)\n",
        "    return score\n",
        "\n",
        "def optuna_perf_wrapper(X, y, meta_params, zerotune_warmstart=None, random_seed=42, n_trials=100):\n",
        "    all_dataset_meta_params = calculate_dataset_meta_parameters(X, y)\n",
        "    dataset_meta_params = {key: all_dataset_meta_params[key] for key in meta_params}\n",
        "    dataset_meta_params_inc_dependencies = {key: all_dataset_meta_params[key] for key in ['n_samples']}\n",
        "    dataset_meta_params_inc_dependencies.update(dataset_meta_params)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=random_seed))\n",
        "\n",
        "    if zerotune_warmstart:\n",
        "        print(\"ZeroTune warm start:\", predicted_hyperparams)\n",
        "        # Enqueue the ZeroTune hyperparameters trial\n",
        "        study.enqueue_trial(predicted_hyperparams)\n",
        "\n",
        "    study.optimize(lambda trial: _optuna_objective(trial, X, y, ml_config[\"param_config\"], meta_params, dataset_meta_params_inc_dependencies), n_trials=n_trials)\n",
        "\n",
        "    best_hyperparams = study.best_params\n",
        "    best_hyperparams = _relative2absolute_dict(param_config, dataset_parameters, best_hyperparams)\n",
        "\n",
        "    best_perf, n_seed_scores = evaluate_model(X, y, \"DecisionTreeClassifier4Param\", best_hyperparams, random_seed=random_seed, n_seeds=20)\n",
        "\n",
        "    return {\"best_hyperparams\":best_hyperparams, \"best_perf\": best_perf,\"n_seed_scores\": n_seed_scores\n",
        "            , \"df_trials\":study.trials_dataframe() }"
      ],
      "metadata": {
        "id": "8NGfz9jttl16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train ZeroTune predictor\n",
        "\n",
        "def train_zeroshot_hpo(df, dataset_features, targets, condition_column=None, n_iter=100):\n",
        "    \"\"\"\n",
        "    Trains a one-shot predictor using a Random Forest Multi-Output Regressor, with the option to use groups for cross-validation.\n",
        "\n",
        "    Args:\n",
        "    df (pandas.DataFrame): The DataFrame containing the dataset.\n",
        "    dataset_features (list): A list of column names in df to be used as model features.\n",
        "    targets (list): A list of target column names in df.\n",
        "    condition_column (str, optional): Column name to be used for defining groups in cross-validation. Defaults to None.\n",
        "    n_iter (int, optional): Number of random search iterations. Defaults to 100.\n",
        "\n",
        "    Returns:\n",
        "    RandomForestRegressor: A RandomForestRegressor model fitted to the specified features and targets.\n",
        "    float: The best score from cross-validation.\n",
        "    \"\"\"\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GroupKFold\n",
        "    import numpy as np\n",
        "    from scipy.stats import randint as sp_randint\n",
        "\n",
        "    X = df[dataset_features]\n",
        "    y = df[targets]\n",
        "\n",
        "    param_dist = {\n",
        "        'n_estimators': sp_randint(100, 300),\n",
        "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "        'min_samples_split': sp_randint(2, 11),\n",
        "        'min_samples_leaf': sp_randint(1, 5),\n",
        "        'max_features': ['auto', 'sqrt'],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "    # Initialize the regressor\n",
        "    regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "    # Choose the cross-validation strategy\n",
        "    if condition_column is None:\n",
        "        cv_strategy = 4\n",
        "    else:\n",
        "        groups = df[condition_column]\n",
        "        cv_strategy = GroupKFold(n_splits=4)\n",
        "\n",
        "    # Set up RandomizedSearchCV with the chosen cross-validation strategy\n",
        "    hpo_search = RandomizedSearchCV(regressor, param_distributions=param_dist, cv=cv_strategy,\n",
        "                                    scoring='neg_mean_squared_error', n_jobs=4, n_iter=n_iter, random_state=42)\n",
        "\n",
        "    # Fit the model\n",
        "    # Pass groups to fit method if GroupKFold is used\n",
        "    if condition_column is None:\n",
        "        hpo_search.fit(X, y)\n",
        "    else:\n",
        "        hpo_search.fit(X, y, groups=groups)\n",
        "\n",
        "    # Best parameters and score\n",
        "    best_params = hpo_search.best_params_\n",
        "    best_score = hpo_search.best_score_\n",
        "\n",
        "    print(f'Zero-shot predictor mse: {best_score}')\n",
        "\n",
        "    # Train the model on the entire dataset with best parameters\n",
        "    best_regressor = RandomForestRegressor(**best_params, random_state=42)\n",
        "    best_regressor.fit(X, y)\n",
        "\n",
        "    return best_regressor, best_score\n",
        "\n",
        "def predict_hyperparameters(model, X, target_columns):\n",
        "\n",
        "    # Make predictions using the trained model\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Create a dictionary mapping target column names to their predicted values\n",
        "    predictions_dict = {column: prediction for column, prediction in zip(target_columns, predictions[0])}\n",
        "\n",
        "    return predictions_dict\n",
        "\n",
        "def remove_param_prefix(param_dict):\n",
        "    return {key.replace('params_', ''): value for key, value in param_dict.items()}\n"
      ],
      "metadata": {
        "id": "3zZfA47CuM1T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build a HPO knowledge base using binary classification datasets\n",
        "\n",
        "verbose = False\n",
        "\n",
        "dataset_id_list = [\n",
        "    31, 38, 44, 52, 151, 179, 298, 846, 917,\n",
        "    1049, 1053, 1111, 1112, 1120, 1128, 1220, 1464,\n",
        "    1494, 1510, 1558, 4534, 23381, 40536, 40900, 45038\n",
        "]\n",
        "\n",
        "dataset_features_list = []\n",
        "for dataset_id in tqdm(dataset_id_list, desc=\"Processing datasets\"):\n",
        "    # Fetch and prepare the dataset\n",
        "    dataset_df, target_name, dataset_name = fetch_open_ml_data(dataset_id)\n",
        "    X, y = prepare_data(dataset_df, target_name)\n",
        "\n",
        "    # Step 1: Generate dataset meta parameters\n",
        "    dataset_parameters = calculate_dataset_meta_parameters(X, y)\n",
        "    if verbose:\n",
        "        print(f'\\nDataset id {dataset_id}, Dataset properties: {dataset_parameters}')\n",
        "    dataset_parameters['Dataset'] = dataset_id\n",
        "    dataset_parameters['DatasetName'] = dataset_name\n",
        "    dataset_features_list.append(dataset_parameters)\n",
        "\n",
        "    # Save dataset parameters to file\n",
        "    pd.to_pickle(dataset_parameters, dataset_features_file)\n",
        "\n",
        "df_dataset_features_realworld = pd.DataFrame(dataset_features_list)\n",
        "\n",
        "df_dataset_features_realworld.to_pickle(os.path.join(output_root, 'dataset_features_all.pkl'))\n",
        "df_dataset_features_realworld.to_csv(f\"{output_root}realworld_dataset_features.csv\", index=False)"
      ],
      "metadata": {
        "id": "bXhYSJoQuOfs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build knowledge base\n",
        "\n",
        "\n",
        "N_SEEDS = 50\n",
        "N_TRIALS = 25\n",
        "\n",
        "dataset_id_list = [\n",
        "    31, 38, 44, 52, 151, 179, 298, 846, 917,\n",
        "    1049, 1053, 1111, 1112, 1120, 1128, 1220, 1464,\n",
        "    1494, 1510, 1558, 4534, 23381, 40536, 40900, 45038\n",
        "]\n",
        "\n",
        "verbose = False\n",
        "checkpoint_dir = os.path.join(output_root, 'checkpoint')\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Initialize lists to store the results\n",
        "dataset_features_list = []\n",
        "optuna_trials_df_list = []\n",
        "\n",
        "for dataset_id in tqdm(dataset_id_list, desc=\"Processing datasets\"):\n",
        "    # Define file paths for checkpointing in the checkpoint directory\n",
        "    dataset_features_file = os.path.join(checkpoint_dir, f'dataset_{dataset_id}_features.pkl')\n",
        "    optuna_trials_file = os.path.join(checkpoint_dir, f'dataset_{dataset_id}_optuna_trials.pkl')\n",
        "\n",
        "    # Check if results for this dataset already exist\n",
        "    if os.path.exists(dataset_features_file) and os.path.exists(optuna_trials_file):\n",
        "        print(f\"Dataset {dataset_id} already processed. Loading results.\")\n",
        "        # Load existing results\n",
        "        dataset_parameters = pd.read_pickle(dataset_features_file)\n",
        "        dataset_features_list.append(dataset_parameters)\n",
        "        optuna_trials = pd.read_pickle(optuna_trials_file)\n",
        "        optuna_trials_df_list.append(optuna_trials)\n",
        "        continue\n",
        "\n",
        "    # Fetch and prepare the dataset\n",
        "    dataset_df, target_name, dataset_name = fetch_open_ml_data(dataset_id)\n",
        "    X, y = prepare_data(dataset_df, target_name)\n",
        "\n",
        "    # Step 1: Generate dataset meta parameters\n",
        "    dataset_parameters = calculate_dataset_meta_parameters(X, y)\n",
        "    if verbose:\n",
        "        print(f'\\nDataset id {dataset_id}, Dataset properties: {dataset_parameters}')\n",
        "    dataset_parameters['Dataset'] = dataset_id\n",
        "    dataset_parameters['DatasetName'] = dataset_name\n",
        "    dataset_features_list.append(dataset_parameters)\n",
        "\n",
        "    # Save dataset parameters to file\n",
        "    pd.to_pickle(dataset_parameters, dataset_features_file)\n",
        "\n",
        "    # Step 2: Perform Optuna HPO - only 'n_samples', 'n_features' are really required for fractional HP representation\n",
        "    dataset_features = ['n_samples', 'n_features', 'n_highly_target_corr']\n",
        "\n",
        "    # Process seeds individually with checkpointing\n",
        "    per_seed_results = []\n",
        "    for seed in range(N_SEEDS):\n",
        "        seed_trials_file = os.path.join(checkpoint_dir, f'dataset_{dataset_id}_seed_{seed}_optuna_trials.pkl')\n",
        "        if os.path.exists(seed_trials_file):\n",
        "            print(f\"Dataset {dataset_id}, Seed {seed} already processed. Loading results.\")\n",
        "            seed_trials = pd.read_pickle(seed_trials_file)\n",
        "        else:\n",
        "            # Run Optuna HPO for the current seed\n",
        "            optuna_output = optuna_perf_wrapper_inc_seeds(\n",
        "                X, y, dataset_features,\n",
        "                n_trials=N_TRIALS, n_seeds=1, seed=seed\n",
        "            )\n",
        "            seed_trials = optuna_output[\"combined_trials_df\"]\n",
        "            seed_trials['Dataset'] = dataset_id\n",
        "            seed_trials['Seed'] = seed\n",
        "            # Save individual seed results\n",
        "            pd.to_pickle(seed_trials, seed_trials_file)\n",
        "        per_seed_results.append(seed_trials)\n",
        "\n",
        "    # Combine seed results for the current dataset\n",
        "    optuna_trials = pd.concat(per_seed_results, ignore_index=True)\n",
        "    optuna_trials_df_list.append(optuna_trials)\n",
        "    # Save combined Optuna trials for the dataset\n",
        "    pd.to_pickle(optuna_trials, optuna_trials_file)\n",
        "\n",
        "# Combine all dataset features and Optuna trials\n",
        "df_dataset_features_realworld = pd.DataFrame(dataset_features_list)\n",
        "df_optuna_trials_all_realworld = pd.concat(optuna_trials_df_list, ignore_index=True)\n",
        "\n",
        "# Save the combined dataset features and Optuna trials\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "df_dataset_features_realworld.to_pickle(os.path.join(output_root, 'dataset_features_all.pkl'))\n",
        "df_optuna_trials_all_realworld.to_pickle(os.path.join(output_root, 'optuna_trials_all.pkl'))\n",
        "\n",
        "# Save csv results\n",
        "df_dataset_features_realworld.to_csv(f\"{output_root}realworld_dataset_features.csv\", index=False)\n",
        "df_optuna_trials_all_realworld.to_csv(f\"{output_root}realworld_kb_optuna_trials_all.csv\", index=False)"
      ],
      "metadata": {
        "id": "YiTJ_3jb7iXD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train ZeroTune Model with recursive feature selection to automactically select the best meta parameters.\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define target hyperparameters and dataset features\n",
        "targets = ['params_max_depth', 'params_max_features', 'params_min_samples_leaf', 'params_min_samples_split']\n",
        "dataset_features = df_dataset_features.columns.difference(['Dataset'])\n",
        "\n",
        "training_dataset_list = [\n",
        "31, 38, 44, 52, 151, 179, 298, 846, 917,\n",
        "1049, 1053, 1111, 1112, 1120, 1128, 1220, 1464,\n",
        "1494, 1510, 1558, 4534, 23381, 40536, 40900, 45038\n",
        "]\n",
        "\n",
        "print(training_dataset_list)\n",
        "\n",
        "df_kb_optuna_trials['rank'] = df_kb_optuna_trials.groupby('Dataset')['value'].rank(method='dense', ascending=False)\n",
        "df = pd.merge(df_kb_optuna_trials, df_dataset_features_realworld, on='Dataset', how='inner')\n",
        "train_df = df[df['Dataset'].isin(training_dataset_list)]\n",
        "\n",
        "print(f'Number of unique datasets used for model training: {len(train_df.Dataset.unique())}')\n",
        "print(f'Number of data points used for model training: {len(train_df)}')\n",
        "\n",
        "# Feature matrix (X) and target matrix (y)\n",
        "X = train_df[dataset_features]\n",
        "y = train_df[targets]\n",
        "\n",
        "# Function to perform RFE with hyperparameter tuning\n",
        "def select_best_features_with_tuning(X, y):\n",
        "    # Hyperparameter tuning for RandomForestRegressor\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "    }\n",
        "\n",
        "    base_model = RandomForestRegressor(random_state=42)\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    # Best model from the grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # RFECV with the tuned model using KFold cross-validation\n",
        "    rfecv = RFECV(estimator=best_model, step=1, cv=KFold(n_splits=5), scoring='neg_mean_squared_error')\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('feature_selection', rfecv),\n",
        "        ('regressor', best_model)\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X, y)\n",
        "    best_features = X.columns[rfecv.support_]\n",
        "    return best_features, rfecv.cv_results_['mean_test_score']\n",
        "\n",
        "# Select the best features\n",
        "best_features, scores = select_best_features_with_tuning(X, y)\n",
        "\n",
        "print('Best features for the multi-output regression model:')\n",
        "print(best_features)\n",
        "\n",
        "print('Cross-validation scores:')\n",
        "print(scores)\n",
        "\n",
        "\n",
        "targets = ['params_max_depth', 'params_max_features', 'params_min_samples_leaf', 'params_min_samples_split']\n",
        "model, error_score = train_zeroshot_hpo(train_df, best_features, targets, condition_column='Dataset')\n",
        "\n",
        "# Save ZeroTune model\n",
        "joblib.dump(model, f\"{output_root}pretrained_models/ZeroTune_{ml_config['name']}_{KB_TYPE}_kb.joblib\")"
      ],
      "metadata": {
        "id": "RvbKOZY87lPK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-w_ArOul-a8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}