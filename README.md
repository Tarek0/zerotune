# ZeroTune

ZeroTune provides **instant zero-shot hyperparameter optimization** using advanced pre-trained models. Get competitive hyperparameters for your machine learning models in sub-millisecond time with robust performance across diverse datasets!

ğŸ† **Decision Tree: 100% win rate** â€¢ ğŸŒ² **Random Forest: 100% win rate** â€¢ ğŸ”§ **XGBoost: 100% win rate** â€¢ ğŸš€ **+7.08%, +1.47% & +0.80% improvements** â€¢ âš¡ **<1ms prediction** â€¢ ğŸ“Š **50-seed validated**

## ğŸš€ Quick Start

```python
from zerotune import ZeroTunePredictor
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

# Load your dataset
df = pd.read_csv('your_dataset.csv')
X = df.drop('target', axis=1)
y = df['target']

# Get optimal hyperparameters instantly
predictor = ZeroTunePredictor(model_name='decision_tree', task_type='binary')
best_params = predictor.predict(X, y)

# Train model with predicted hyperparameters
model = DecisionTreeClassifier(**best_params)
model.fit(X, y)

print(f"Optimal hyperparameters: {best_params}")
# Expected: +7.08% improvement over random hyperparameters
```

## âœ¨ Key Features

- **ğŸ† 100% Win Rate**: All three models (Decision Tree, Random Forest, XGBoost) beat random hyperparameters on every test dataset
- **âš¡ Instant Predictions**: Sub-millisecond hyperparameter optimization (vs hours of traditional HPO)
- **ğŸ¯ Significant Improvements**: +7.08%, +1.47%, +0.80% average performance gains respectively
- **ğŸ”¬ Scientifically Validated**: 50-seed evaluation across diverse datasets with statistical rigor
- **ğŸš€ Production Ready**: Pre-trained models included - no training required
- **ğŸ”§ Optuna Integration**: Warm-start TPE optimization with perfect baseline consistency

## ğŸ¯ Supported Models

| Model | Binary Classification | Performance |
|-------|----------------------|-------------|
| **ğŸ† Decision Tree** | âœ… | **100% win rate, +7.08%** |
| **ğŸŒ² Random Forest** | âœ… | **100% win rate, +1.47%** |
| **ğŸ”§ XGBoost** | âœ… | **100% win rate, +0.80%** |

**All models achieve 100% win rates** - every single prediction outperforms random hyperparameter selection.

## ğŸ“¦ Installation

```bash
# Install Poetry (if not already installed)
curl -sSL https://install.python-poetry.org | python3 -

# Install ZeroTune
git clone https://github.com/your-repo/zerotune.git
cd zerotune
poetry install
```

ğŸš€ **Ready-to-Use**: All trained models are included - start predicting immediately!

## ğŸ”§ Usage

### Zero-Shot Predictions (Main Use Case)

```python
from zerotune import ZeroTunePredictor

# For different models
predictor_dt = ZeroTunePredictor(model_name='decision_tree', task_type='binary')
predictor_rf = ZeroTunePredictor(model_name='random_forest', task_type='binary')
predictor_xgb = ZeroTunePredictor(model_name='xgboost', task_type='binary')

# Get instant predictions
hyperparams = predictor_dt.predict(X, y)
```

### Optuna TPE Warm-Start

```python
from zerotune.core.optimization import optimize_hyperparameters

# Use zero-shot predictions to warm-start Optuna TPE
best_params, study = optimize_hyperparameters(
    X=X, y=y,
    model_type='decision_tree',
    param_grid=param_grid,
    n_trials=20,
    warm_start=True,  # Uses ZeroTune predictions
    n_jobs=1
)
```

### Command Line Interface

```bash
# Quick evaluation on test datasets
poetry run python decision_tree_experiment.py eval-test
poetry run python random_forest_experiment.py eval-test  
poetry run python xgb_experiment.py eval-test

# Full evaluation with Optuna benchmarking
poetry run python decision_tree_experiment.py eval-full --optuna --optuna_trials 25 --seeds 50
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Knowledge Base    â”‚â”€â”€â”€â–¶â”‚   Pre-trained Model  â”‚â”€â”€â”€â–¶â”‚  Zero-Shot Predict  â”‚
â”‚   Building          â”‚    â”‚   Training           â”‚    â”‚  (ZeroTunePredictor)â”‚
â”‚   (ZeroTune)        â”‚    â”‚                      â”‚    â”‚                     â”‚
â”‚                     â”‚    â”‚                      â”‚    â”‚                     â”‚
â”‚ â€¢ Multi-seed HPO on â”‚    â”‚ â€¢ RFECV feature      â”‚    â”‚ â€¢ Sub-ms prediction â”‚
â”‚   many datasets     â”‚    â”‚   selection (15/22)  â”‚    â”‚ â€¢ 100% win rate     â”‚
â”‚ â€¢ Extract 22+ meta- â”‚    â”‚ â€¢ Top-K filtering    â”‚    â”‚ â€¢ Feature selection â”‚
â”‚   features          â”‚    â”‚ â€¢ RandomForest +HPO  â”‚    â”‚ â€¢ High performance  â”‚
â”‚ â€¢ Store full trials â”‚    â”‚ â€¢ Meta-features â†’    â”‚    â”‚                     â”‚
â”‚   dataframes        â”‚    â”‚   Hyperparameters    â”‚    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                     â”‚
                                                                     â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Optuna TPE         â”‚â—€â”€â”€â”€â”‚  Your ML Pipeline   â”‚
                           â”‚   Warm-Start         â”‚    â”‚                     â”‚
                           â”‚                      â”‚    â”‚                     â”‚
                           â”‚ â€¢ Zero-shot init     â”‚    â”‚ â€¢ Train your model  â”‚
                           â”‚ â€¢ Faster convergence â”‚    â”‚ â€¢ Better performanceâ”‚
                           â”‚ â€¢ study.enqueue()    â”‚    â”‚ â€¢ Production deploy â”‚
                           â”‚ â€¢ Perfect baseline   â”‚    â”‚ â€¢ Instant results   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Knowledge Base**: Multi-dataset HPO experiments with 22+ meta-features extracted
2. **Model Training**: RFECV feature selection + RandomForest predictor with hyperparameter optimization  
3. **Zero-Shot Prediction**: Instant hyperparameter prediction based on dataset characteristics
4. **Optional Warm-Start**: Use predictions to initialize Optuna TPE for further optimization

## ğŸ“Š Performance Summary

### Quick Results Overview

| Model | Win Rate | Avg Improvement | Best Single Win | Statistical Significance |
|-------|----------|-----------------|-----------------|-------------------------|
| **Decision Tree** | **100%** | **+7.08%** | +17.4% | 90% of datasets |
| **Random Forest** | **100%** | **+1.47%** | +4.4% | 50% of datasets |
| **XGBoost** | **100%** | **+0.80%** | +2.6% | 90% of datasets |

**Key Benefits**:
- âœ… **Perfect Reliability**: 100% win rate across all models and test datasets
- âœ… **Instant Results**: Sub-millisecond prediction vs hours of traditional HPO
- âœ… **Statistical Rigor**: 50 random seeds Ã— 10 datasets = 500 total experiments
- âœ… **Production Ready**: No training required, robust error handling

*For detailed performance analysis, see [PERFORMANCE_ANALYSIS.md](PERFORMANCE_ANALYSIS.md)*

## ğŸ“ˆ Research & Publication

For researchers and advanced users:

```bash
# Generate publication-ready analysis and charts
poetry run python publication_analysis.py DecisionTree --auto-detect
poetry run python publication_analysis.py RandomForest --auto-detect
poetry run python publication_analysis.py XGBoost --auto-detect
```

See [PUBLICATION_CHARTS_GUIDE.md](PUBLICATION_CHARTS_GUIDE.md) for detailed documentation.

## ğŸ› ï¸ Advanced Usage

### Building Custom Knowledge Bases

```python
from zerotune import ZeroTune

# Build knowledge base from your datasets
zt = ZeroTune(model_type='xgboost', kb_path='my_knowledge_base.json')
dataset_ids = [31, 38, 44, 52, 151]  # OpenML dataset IDs
kb = zt.build_knowledge_base(dataset_ids=dataset_ids, n_iter=20)
```

### Training New Predictors

```python
from zerotune.core.predictor_training import train_predictor_from_knowledge_base

# Train predictor from knowledge base
model_path = train_predictor_from_knowledge_base(
    kb_path='my_knowledge_base.json',
    model_name='xgboost',
    task_type='binary',
    top_k_per_seed=3
)
```

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for development setup and contribution guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. 